from flask import Flask, render_template, request, jsonify, render_template_string, Response, make_response
import requests
import concurrent.futures
import sqlite3
import datetime
import os
import threading
import pytz
import urllib3
import time
import io
import csv
from collections import Counter
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

# --- æ–°å¢ï¼šå¼•å…¥é™æµåº“ ---
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address

# ç¦ç”¨å®‰å…¨è¯·æ±‚è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

app = Flask(__name__)

# ================= æ–°å¢ï¼šé˜²å¾¡æœºåˆ¶é…ç½® (æ”¾åœ¨ app åˆå§‹åŒ–çš„æ­£ä¸‹æ–¹) =================

# 1. å®šä¹‰è·å–çœŸå®IPçš„å‡½æ•° (é€‚é… Render ä»£ç†ç¯å¢ƒï¼Œé˜²æ­¢è¯¯æ€)
def get_real_ip():
    if request.headers.getlist("X-Forwarded-For"):
        return request.headers.getlist("X-Forwarded-For")[0]
    return get_remote_address()

# 2. åˆå§‹åŒ–é™æµå™¨
limiter = Limiter(
    key_func=get_real_ip,
    app=app,
    default_limits=["2000 per day", "500 per hour"], # é»˜è®¤å…¨ç«™æ¯äººæ¯å¤©ä¸Šé™ï¼Œé˜²æç«¯çˆ¬è™«
    storage_uri="memory://"
)

# ================= é…ç½®åŒºåŸŸ =================

HOME_NOTICE = {
    "enabled": True,
    "version": "v5.0",
    "title": "âš¡ ç³»ç»Ÿå‡çº§å®Œæ¯•",
    "content": """
    <p>1. <b>åå°å‡çº§</b>ï¼šæ–°å¢å¯è§†åŒ–æ•°æ®çœ‹æ¿ï¼Œæµé‡ä¸€ç›®äº†ç„¶ã€‚</p>
    <p>2. <b>æ™ºèƒ½æ—¥å¿—</b>ï¼šä¼˜åŒ–äº†æ’­æ”¾è®°å½•ï¼Œç²¾ç¡®ç»Ÿè®¡çƒ­é—¨ç‰‡å•ã€‚</p>
    <p>3. <b>æ€§èƒ½ç»´æŒ</b>ï¼šåœ¨å¢å¼ºåŠŸèƒ½çš„åŒæ—¶ï¼Œä¿æŒäº†æé€Ÿå“åº”å†…æ ¸ã€‚</p>
    """
}

# ================= 0. åº•å±‚ç½‘ç»œä¸ç¼“å­˜ =================
GLOBAL_SESSION = requests.Session()
retry = Retry(connect=2, read=2, backoff_factor=0.1, status_forcelist=[500, 502, 503, 504])
adapter = HTTPAdapter(max_retries=retry, pool_connections=20, pool_maxsize=20)
GLOBAL_SESSION.mount('http://', adapter)
GLOBAL_SESSION.mount('https://', adapter)
GLOBAL_SESSION.headers.update({
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
})


class SimpleCache:
    def __init__(self, ttl_seconds=600):
        self.cache = {}
        self.ttl = ttl_seconds
        self.lock = threading.Lock()

    def get(self, key):
        with self.lock:
            if key in self.cache:
                timestamp, data = self.cache[key]
                if time.time() - timestamp < self.ttl:
                    return data
                else:
                    del self.cache[key]
        return None

    def set(self, key, data):
        with self.lock:
            if len(self.cache) > 1000: self.cache.clear()
            self.cache[key] = (time.time(), data)


search_cache = SimpleCache(ttl_seconds=1800)

# ================= 1. åŸºç¡€é…ç½®ä¸æ•°æ®åº“ =================
DB_FILE = 'site_stats.db'


def init_db():
    with sqlite3.connect(DB_FILE) as conn:
        c = conn.cursor()
        c.execute('''CREATE TABLE IF NOT EXISTS visits 
                     (id INTEGER PRIMARY KEY AUTOINCREMENT, 
                      ip TEXT, location TEXT, time TIMESTAMP, endpoint TEXT, user_agent TEXT)''')
        try:
            c.execute("SELECT location FROM visits LIMIT 1")
        except:
            c.execute("ALTER TABLE visits ADD COLUMN location TEXT")
        conn.commit()


init_db()


def log_traffic(endpoint, extra_info=None):
    try:
        if request.headers.getlist("X-Forwarded-For"):
            ip = request.headers.getlist("X-Forwarded-For")[0]
        else:
            ip = request.remote_addr
        ua = request.headers.get('User-Agent', '')

        if 'UptimeRobot' in ua:
            threading.Thread(target=simple_logger, args=(ip, endpoint, ua)).start()
            return

        full_action = endpoint
        if extra_info:
            full_action = f"{endpoint} | {extra_info}"

        threading.Thread(target=background_logger, args=(ip, full_action, ua)).start()
    except:
        pass


def simple_logger(ip, endpoint, ua):
    try:
        now = datetime.datetime.now(pytz.timezone('Asia/Shanghai')).strftime("%Y-%m-%d %H:%M:%S")
        with sqlite3.connect(DB_FILE) as conn:
            conn.cursor().execute(
                "INSERT INTO visits (ip, location, time, endpoint, user_agent) VALUES (?, ?, ?, ?, ?)",
                (ip, "UptimeRobot", now, endpoint, ua))
            conn.commit()
    except:
        pass


def background_logger(ip, endpoint, user_agent):
    location = "æœªçŸ¥"
    try:
        if not ip.startswith('127.') and not ip.startswith('192.168.'):
            resp = GLOBAL_SESSION.get(f"http://ip-api.com/json/{ip}?lang=zh-CN", timeout=2, verify=False)
            if resp.status_code == 200 and resp.json()['status'] == 'success':
                d = resp.json()
                location = f"{d['country']} {d['regionName']} {d['city']}"
    except:
        pass

    try:
        now = datetime.datetime.now(pytz.timezone('Asia/Shanghai')).strftime("%Y-%m-%d %H:%M:%S")
        with sqlite3.connect(DB_FILE) as conn:
            conn.cursor().execute(
                "INSERT INTO visits (ip, location, time, endpoint, user_agent) VALUES (?, ?, ?, ?, ?)",
                (ip, location, now, endpoint, user_agent))
            conn.commit()
    except:
        pass


# ================= 2. è§†é¢‘æºé€»è¾‘ =================
DIRECT_SOURCES = [
    {"name": "é‡å­èµ„æº", "api": "https://cj.lziapi.com/api.php/provide/vod/", "speed": "ğŸš€ æé€Ÿ", "primary": True},
    {"name": "éå‡¡èµ„æº", "api": "http://cj.ffzyapi.com/api.php/provide/vod/", "speed": "ğŸ¢ ç¨³å®š", "primary": False},
    {"name": "æš´é£èµ„æº", "api": "https://bfzyapi.com/api.php/provide/vod", "speed": "âš¡ é«˜é€Ÿ", "primary": False}
]


def normalize_type(raw_type):
    if not raw_type: return "å…¶ä»–"
    if any(k in raw_type for k in ['ç”µå½±', 'ç‰‡', 'å‰§åœº']): return "ç”µå½±"
    if any(k in raw_type for k in ['å‰§', 'è¿ç»­', 'é›†']): return "ç”µè§†å‰§"
    if any(k in raw_type for k in ['åŠ¨æ¼«', 'åŠ¨ç”»']): return "åŠ¨æ¼«"
    if any(k in raw_type for k in ['ç»¼è‰º', 'ç§€']): return "ç»¼è‰º"
    if any(k in raw_type for k in ['çŸ­å‰§']): return "çŸ­å‰§"
    return "å…¶ä»–"


def fetch_single_source_search(source, keyword):
    try:
        resp = GLOBAL_SESSION.get(source['api'], params={"ac": "list", "wd": keyword}, timeout=5)
        data = resp.json()
        video_list = data.get("list") or data.get("data")
        results = []
        if video_list:
            for i in video_list:
                name = i.get("vod_name", "æœªçŸ¥")
                if "ç¦åˆ©" in name or "ä¼¦ç†" in name: continue
                results.append({
                    "id": i["vod_id"], "title": name, "img": i.get("vod_pic"),
                    "note": i.get("vod_remarks", ""), "api": source['api'],
                    "source_name": source['name'], "speed": source.get('speed', 'æœªçŸ¥'),
                    "type": normalize_type(i.get("type_name", "")), "raw_type": i.get("type_name", "")
                })
        return results
    except:
        return []


def search_global(keyword, mode='fast'):
    cache_key = f"{keyword}_{mode}"
    cached_data = search_cache.get(cache_key)
    if cached_data: return cached_data

    all_movies = []
    if mode == 'fast':
        target_sources = [DIRECT_SOURCES[0]]
    else:
        target_sources = DIRECT_SOURCES

    with concurrent.futures.ThreadPoolExecutor(max_workers=len(target_sources)) as executor:
        futures = [executor.submit(fetch_single_source_search, src, keyword) for src in target_sources]
        for future in concurrent.futures.as_completed(futures):
            res = future.result()
            if res: all_movies.extend(res)

    all_movies.sort(key=lambda x: (x['title'] != keyword, len(x['title']), x['title']))
    search_cache.set(cache_key, all_movies)
    return all_movies


def get_video_details(api_url, vod_id):
    try:
        resp = GLOBAL_SESSION.get(api_url, params={"ac": "detail", "ids": vod_id}, timeout=6)
        data = resp.json()
        video_list = data.get("list") or data.get("data")
        if video_list:
            info = video_list[0]
            play_url_str = info.get("vod_play_url", "")
            target_play_url = ""
            chunks = play_url_str.split("$$$")
            found_m3u8 = False
            for chunk in chunks:
                if ".m3u8" in chunk:
                    target_play_url = chunk;
                    found_m3u8 = True;
                    break
            if not found_m3u8 and chunks: target_play_url = chunks[0]
            episodes = []
            if target_play_url:
                for idx, item in enumerate(target_play_url.split("#")):
                    parts = item.split("$")
                    if len(parts) >= 2:
                        name = parts[-2]; url = parts[-1]
                    else:
                        name = f"ç¬¬{idx + 1}é›†"; url = parts[0]
                    if url.endswith(".m3u8") or url.endswith(".mp4"):
                        episodes.append({"index": idx, "name": name, "url": url})
            return {
                "id": info["vod_id"], "title": info["vod_name"],
                "desc": info.get("vod_content", "").replace('<p>', '').replace('</p>', ''),
                "pic": info.get("vod_pic"), "episodes": episodes, "api": api_url,
                "type_name": normalize_type(info.get("type_name", ""))
            }
    except:
        pass
    return None


# ================= 3. æ•°æ®åˆ†æé€»è¾‘ =================
def get_dashboard_stats():
    stats = {
        'today_pv': 0, 'today_uv': 0, 'total_logs': 0,
        'top_search': [], 'top_play': [], 'recent_logs': [],
        'chart_labels': [], 'chart_data': []
    }
    try:
        conn = sqlite3.connect(DB_FILE)
        c = conn.cursor()

        # 1. åŸºç¡€æ€»æ•°
        c.execute("SELECT COUNT(*) FROM visits")
        stats['total_logs'] = c.fetchone()[0]

        # 2. ä»Šæ—¥æ•°æ®
        today_str = datetime.datetime.now(pytz.timezone('Asia/Shanghai')).strftime("%Y-%m-%d")
        c.execute("SELECT COUNT(*) FROM visits WHERE time LIKE ?", (f"{today_str}%",))
        stats['today_pv'] = c.fetchone()[0]

        c.execute("SELECT COUNT(DISTINCT ip) FROM visits WHERE time LIKE ?", (f"{today_str}%",))
        stats['today_uv'] = c.fetchone()[0]

        # 3. æœ€è¿‘20æ¡æ—¥å¿—
        c.execute("SELECT time, ip, location, endpoint FROM visits ORDER BY time DESC LIMIT 20")
        stats['recent_logs'] = [{'time': r[0].split(' ')[1], 'ip': r[1], 'loc': r[2], 'act': r[3]} for r in
                                c.fetchall()]

        # 4. å›¾è¡¨æ•°æ® (è¿‡å»7å¤©PV)
        dates = []
        counts = []
        for i in range(6, -1, -1):
            d = (datetime.datetime.now(pytz.timezone('Asia/Shanghai')) - datetime.timedelta(days=i)).strftime(
                "%Y-%m-%d")
            c.execute("SELECT COUNT(*) FROM visits WHERE time LIKE ?", (f"{d}%",))
            cnt = c.fetchone()[0]
            dates.append(d[5:])  # åªå– MM-DD
            counts.append(cnt)
        stats['chart_labels'] = dates
        stats['chart_data'] = counts

        # 5. çƒ­é—¨æœç´¢ & æ’­æ”¾ (å–æœ€è¿‘1000æ¡åˆ†æï¼Œé¿å…å¤ªæ…¢)
        c.execute("SELECT endpoint FROM visits ORDER BY time DESC LIMIT 1000")
        rows = c.fetchall()
        search_words = []
        play_names = []
        for r in rows:
            act = r[0]
            if 'æœç´¢:' in act:
                search_words.append(act.split('æœç´¢:')[1].strip())
            if 'æ’­æ”¾:' in act:
                # å°è¯•æå–ç‰‡å
                if '(' in act and ')' in act:
                    play_names.append(act.split('æ’­æ”¾:')[1].split('(')[0].strip())
                else:
                    play_names.append(act)  # æ—§æ•°æ®æ ¼å¼

        stats['top_search'] = Counter(search_words).most_common(8)
        stats['top_play'] = Counter(play_names).most_common(8)

        conn.close()
    except Exception as e:
        print(f"Stats Error: {e}")
    return stats


# ================= 4. è·¯ç”± =================
@app.route('/')
def home():
    log_traffic('é¦–é¡µè®¿é—®')
    try:
        with open('templates/index.html', 'r', encoding='utf-8') as f:
            html_content = f.read()
        if HOME_NOTICE['enabled']:
            pass
        return render_template_string(html_content)
    except:
        return render_template('index.html')


@app.route('/api/search_json')
@limiter.limit("8 per minute")  # ğŸ”’ ä¿®æ”¹ç‚¹ï¼šAPIé™æµï¼Œæ¯åˆ†é’Ÿæ¯IPæœ€å¤š10æ¬¡
def search_json_handler():
    keyword = request.args.get('keyword')
    if not keyword: return jsonify([])
    movies = search_global(keyword, mode='all')
    return jsonify(movies)


@app.route('/api/cover_rescue')
def cover_rescue_handler():
    return jsonify({'url': ''})


@app.route('/search', methods=['POST', 'GET'])
@limiter.limit("8 per minute")   # ğŸ”’ ä¿®æ”¹ç‚¹ï¼šæœç´¢é™æµï¼Œæ¯åˆ†é’Ÿæ¯IPæœ€å¤š8æ¬¡
def search_handler():
    keyword = request.form.get('keyword') or request.args.get('keyword')
    if not keyword: return render_template('index.html')
    log_traffic(f'æœç´¢: {keyword}')
    movies = search_global(keyword, mode='fast')
    return render_template('results.html', movies=movies, keyword=keyword)


@app.route('/play')
def play_handler():
    vod_id = request.args.get('id')
    ep_index = request.args.get('ep_index', 0, type=int)
    api = request.args.get('api')

    # å…³é”®ä¿®æ”¹ï¼šè·å–è§†é¢‘ä¿¡æ¯åå†è®°å½•æ—¥å¿—ï¼Œè¿™æ ·æ—¥å¿—é‡Œå°±æœ‰ç‰‡åäº†
    video_data = get_video_details(api, vod_id)

    if video_data:
        # è®°å½•ç‰‡åï¼Œæ–¹ä¾¿åå°ç»Ÿè®¡
        log_traffic(f'æ’­æ”¾: {video_data["title"]} (ID-{vod_id})')

        if video_data.get('episodes'):
            if ep_index >= len(video_data['episodes']): ep_index = 0
            return render_template('player.html', video=video_data, current_ep=video_data['episodes'][ep_index],
                                   current_index=ep_index, current_api=api)

    log_traffic(f'æ’­æ”¾å¤±è´¥: ID-{vod_id}')
    return "<h3>âš ï¸ è§†é¢‘åŠ è½½å¤±è´¥ï¼Œè¯·è¿”å›é‡è¯•ã€‚</h3>"


@app.route('/api/heartbeat', methods=['POST', 'GET'])
def heartbeat():
    return "ok"


# --- ç®¡ç†åå° (å·²ä¿®å¤å®‰å…¨æ¼æ´) ---
@app.route('/admin')
def admin_dashboard():
    # --- ğŸ”’ åªæœ‰æºå¸¦æ­£ç¡®å‚æ•°æ‰èƒ½è®¿é—® ---
    password = request.args.get('pass')
    if password != 'Zzk1810342428!':  # æ‚¨çš„ä¸“å±å¯†ç 
        return "<h1>ğŸš« 521 love you - ä»€ä¹ˆéƒ½æ²¡æœ‰ï¼Œåˆ«è¯•äº†</h1>", 403
    # -----------------------------------

    stats = get_dashboard_stats()
    return render_template('admin.html', stats=stats)


@app.route('/admin/export_csv')
def export_csv_handler():
    # å¯¼å‡ºä¹Ÿå»ºè®®åŠ ä¸€é“é”ï¼Œé˜²æ­¢åˆ«äººçŒœåˆ°URLç›´æ¥å¯¼å‡º
    password = request.args.get('pass')
    if password != 'Zzk1810342428!':
         return "<h1>ğŸš« 403 Forbidden</h1>", 403

    try:
        conn = sqlite3.connect(DB_FILE)
        c = conn.cursor()
        c.execute("SELECT * FROM visits ORDER BY time DESC LIMIT 5000")
        rows = c.fetchall()
        conn.close()
        si = io.StringIO()
        cw = csv.writer(si)
        cw.writerow(['ID', 'IP', 'Location', 'Time', 'Action', 'User-Agent'])
        cw.writerows(rows)
        output = make_response(si.getvalue())
        output.headers["Content-Disposition"] = "attachment; filename=traffic_data.csv"
        output.headers["Content-type"] = "text/csv"
        return output
    except Exception as e:
        return f"Export Error: {e}"


if __name__ == '__main__':
    port = int(os.environ.get('PORT', 5000))
    print(f"ğŸš€ æœåŠ¡å¯åŠ¨: http://0.0.0.0:{port}")
    app.run(host='0.0.0.0', port=port, threaded=True)